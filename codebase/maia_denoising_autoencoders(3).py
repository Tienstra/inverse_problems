# -*- coding: utf-8 -*-
"""Maia_Denoising_Autoencoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Peh4KqVnHM4BLs-il9zJ-_U80Drpy-v5

## Intro
"""

from google.colab import drive 
drive.mount("/content/gdrive")

#clean memory
import gc
try:
    del images
except:
    pass
gc.collect(2)

"""### File Paths"""

image_path = "/content/gdrive/My Drive/Inverse Problems/Images/"
model_path = "/content/gdrive/My Drive/Inverse Problems/Models/"
history_path = "/content/gdrive/My Drive/Inverse Problems/CSV/"
graph_path = "/content/gdrive/My Drive/Inverse Problems/Graphs/"

from keras.layers import Input, Dense
from keras.models import Model
from keras import regularizers
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D
from keras.models import Model
from keras import backend as K
import matplotlib.pyplot as plt

from skimage.metrics import peak_signal_noise_ratio
from scipy.ndimage import gaussian_filter
from numpy import savetxt
from numpy import loadtxt
import os 
import pandas as pd

from keras.datasets import mnist, fashion_mnist
import numpy as np
data_dict = {"mnist": mnist.load_data(), "fashion_mnist": fashion_mnist.load_data()}

"""## Data Set """

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format

"""### Function to add noise/blur"""

#Make noisy images 

def add_noise(x,y,delta):
    x = x + delta * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
    y = y + delta * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 

    x = np.clip(x, 0., 1.)
    y = np.clip(y, 0., 1.)

    return x,y

#Make blurry images  

def add_blur_noise(x,y,delta):
    x = gaussian_filter(x_train, sigma=0.5) + delta * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) 
    y = gaussian_filter(x_test, sigma=0.5)   + delta * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) 
    x = np.clip(x, 0., 1.)
    y = np.clip(y, 0., 1.)

    return x,y

"""### Function to calulate average PSNR"""

def calculate_psnr(u, u_rec, n):
    psnr_vec = np.zeros(n)
    for i in range(0,n): 
        psnr_vec[i] = peak_signal_noise_ratio(u[i].reshape(28,28), u_rec[i].reshape(28,28))
    
    mean = round(np.mean(psnr_vec),2)
    return mean

"""### Function to Plot Loss"""

def plot_loss(history_dic):
    iterations = list(range(1,len(history_dic['loss'])+1))
    plt.plot(iterations,history_dic['loss'],label='training')
    plt.plot(iterations,history_dic['val_loss'],label='validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(graph_path+'loss.png')
    plt.show()

"""### Plot"""

#show orginal images 
n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

#show noisy images
x_train_noisy, x_test_noisy = add_noise(x_train,x_test,0.25)

n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

#show blurry + noisy images
x_train_blurry, x_test_blurry = add_blur_noise(x_train,x_test,0.25)

n = 10
plt.figure(figsize=(20, 2))
for i in range(1, n):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test_blurry[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

"""## Autodenoising with CNN

### The Model
"""

#The model
input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format

x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = MaxPooling2D((2, 2), padding='same')(x)

# at this point the representation is (7, 7, 32)

x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder = Model(input_img, decoded)

#complie model 
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

"""### Training for Noisy data"""

# noise_levels_dic = {'0.1':0.10, '0.2':0.2, '0.3':0.30, '0.4':0.4, 
#                     '0.5': 0.50, '0.6':0.60, '0.7':0.7,'0.8':0.8,
#                     '0.9':0.9, '1':1
#                     }

noise_levels_dic = {'0.1':0.10, '0.2':0.2
                    }

noise_levels_dic
psnr_dic={}

for noise_level, delta in noise_levels_dic.items():
    #make data 
    x_train_noisy, x_test_noisy = add_noise(x_train,x_test,delta)
    save_name = "_".join(('noise_level ',noise_level))
    #fit the model
    print('\n')
    print('\n')
    print('Fitting model on data with noise level '+noise_level)
    print('\n')
    history = autoencoder.fit(x_train_noisy, x_train,
                    epochs=2,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(x_test_noisy, x_test))
    
    #Saves in a record of the training and validation loss
    pd.DataFrame(history.history).to_csv(history_path+save_name+".csv")

    autoencoder.save(model_path+save_name+".h5")

    #evalute the model 
    print('\n')
    print('Evaluating Model')
    print('\n')
    accuracy_train = history.model.evaluate(x_train_noisy,x_train,verbose=0)
    accuracy_test = history.model.evaluate(x_test_noisy,x_test, verbose=0)

    
    plot_loss(history.history)

    #plot
    # encode and decode some digits
    # note that we take them from the *test* set
    print('Plotting')
    print('\n')
    encoded_imgs = autoencoder.predict(x_test_noisy[:100])
    decoded_imgs = autoencoder.predict(encoded_imgs)

    n = 10  # how many digits we will display
    plt.figure(figsize=(20, 4))
    for i in range(1, n):
        # display original noisy
        ax = plt.subplot(2, n, i+1)
        plt.imshow(x_test_noisy[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        

        # display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(decoded_imgs[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

    plt.savefig(image_path+
                save_name+'.png', bbox_inches='tight')
    plt.show()

    #calculate PSNR based on 5000 samples and saves in dictionary
    psnr_dic[noise_level]=calculate_psnr(x_test,x_test_noisy,5000)


# saves to csv file
pd.DataFrame(psnr_dic,index=psnr_dic.keys()).to_csv(history_path+"psnr_noise.csv")

"""### Fit for blurry + noisy data"""

for noise_level, delta in noise_levels_dic.items():
    #make data 
    x_train_noisy, x_test_noisy = add_blur_noise(x_train,x_test,delta)
    save_name = "_".join(('blur_level_0.5_with_noise_level ',noise_level))
    #fit the model
    print('\n')
    print('\n')
    print('Fitting model on data with noise level '+noise_level)
    print('\n')
    history = autoencoder.fit(x_train_noisy, x_train,
                    epochs=2,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(x_test_noisy, x_test))
    
    #Saves in a record of the training and validation loss
    pd.DataFrame(history.history).to_csv(history_path+save_name+".csv")

    autoencoder.save(model_path+save_name+".h5")

    #evalute the model 
    print('\n')
    print('Evaluating Model')
    print('\n')
    accuracy_train = history.model.evaluate(x_train_noisy,x_train,verbose=0)
    accuracy_test = history.model.evaluate(x_test_noisy,x_test, verbose=0)

    
    plot_loss(history.history)

    #plot
    # encode and decode some digits
    # note that we take them from the *test* set
    print('Plotting')
    print('\n')
    encoded_imgs = autoencoder.predict(x_test_noisy[:100])
    decoded_imgs = autoencoder.predict(encoded_imgs)

    n = 10  # how many digits we will display
    plt.figure(figsize=(20, 4))
    for i in range(1, n):
        # display original noisy
        ax = plt.subplot(2, n, i+1)
        plt.imshow(x_test_noisy[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        

        # display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(decoded_imgs[i].reshape(28, 28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

    plt.savefig(image_path+
                save_name+'.png', bbox_inches='tight')
    plt.show()

    #calculate PSNR based on 5000 samples and saves in dictionary
    psnr_dic[noise_level]=calculate_psnr(x_test,x_test_noisy,5000)


# saves to csv file
pd.DataFrame(psnr_dic,index=psnr_dic.keys()).to_csv(history_path+"psnr_blur.csv")

"""## Comarison for noisy data"""

# noisy image
id = 1
f = x_train[id].reshape(28,28)
f_delta = x_train_noisy[id].reshape(28,28)

"""### Perona-Malik regularisation """

# parameters
alpha = 5
dt = 1e-6
niter = 2001
n = 28
coeff = lambda s : 1/(1+1e6*s)

# diffusion operator
def L(u,coeff = lambda s : 1):
    ue = np.pad(u,1,mode='edge') # padd edges to get array of size n+2 x n+2

    # diffusion coefficient (central differences)
    grad_norm = ((ue[2:,1:-1] - ue[:-2,1:-1])/(2/n))**2 + ((ue[1:-1,2:] - ue[1:-1,:-2])/(2/n))**2
    c = np.pad(coeff(grad_norm),1,mode='edge')

    # diffusion term (combination of forward and backward differences)
    uxx = ((c[1:-1,1:-1] + c[2:,1:-1])*(ue[2:,1:-1]-ue[1:-1,1:-1]) - (c[:-2,1:-1]+c[1:-1,1:-1])*(ue[1:-1,1:-1]-ue[:-2,1:-1]))/(2/n**2)
    uyy = ((c[1:-1,1:-1] + c[1:-1,2:])*(ue[1:-1,2:]-ue[1:-1,1:-1]) - (c[1:-1,:-2]+c[1:-1,1:-1])*(ue[1:-1,1:-1]-ue[1:-1,:-2,]))/(2/n**2)

    return uxx + uyy

# solve evolution equation
u = np.zeros((n,n))

for k in range(niter-1):
    u = u - dt*(u - alpha*L(u,coeff)) + dt*f_delta

# plot
fig,ax = plt.subplots(1,2)

ax[0].imshow(f_delta)
ax[0].set_title('Noisy image')
ax[0].set_xticks([])
ax[0].set_yticks([])

ax[1].imshow(u)
ax[1].set_title('Result PM')
ax[1].set_xticks([])
ax[1].set_yticks([])

"""### Non-linear diffusion equation corresponding to the TV-denoising """

# parameters
alpha = 5
dt = 1e-6
niter = 1001
n = 28
coeff = lambda s : 1/((5)+np.sqrt(s))

# diffusion operator.... the finite difference matrix? 
def L(u,coeff = lambda s : 1):
    ue = np.pad(u,1,mode='edge') # padd edges to get array of size n+2 x n+2

    # diffusion coefficient (central differences)
    grad_norm = ((ue[2:,1:-1] - ue[:-2,1:-1])/(2/n))**2 + ((ue[1:-1,2:] - ue[1:-1,:-2])/(2/n))**2
    c = np.pad(coeff(grad_norm),1,mode='edge')

    # diffusion term (combination of forward and backward differences)
    uxx = ((c[1:-1,1:-1] + c[2:,1:-1])*(ue[2:,1:-1]-ue[1:-1,1:-1]) - (c[:-2,1:-1]+c[1:-1,1:-1])*(ue[1:-1,1:-1]-ue[:-2,1:-1]))/(2/n**2)
    uyy = ((c[1:-1,1:-1] + c[1:-1,2:])*(ue[1:-1,2:]-ue[1:-1,1:-1]) - (c[1:-1,:-2]+c[1:-1,1:-1])*(ue[1:-1,1:-1]-ue[1:-1,:-2,]))/(2/n**2)

    return uxx + uyy

# solve evolution equation
u_tv = np.zeros((n,n))

#the central FD in space and Forward Eurler Time
for k in range(niter-1):
    u_tv = u_tv - dt*(u_tv - alpha*L(u_tv,coeff)) + dt*f_delta

# plot
fig,ax = plt.subplots(1,2)

ax[0].imshow(f_delta)
ax[0].set_title('Noisy image')
ax[0].set_xticks([])
ax[0].set_yticks([])

ax[1].imshow(u_tv)
ax[1].set_title('Result NonLin TV')
ax[1].set_xticks([])
ax[1].set_yticks([])

"""## Comparison for blurry + noisy images"""

# make data
id = 1
f_delta = x_train_blurry[id].reshape(28,28)

"""### Proximal Gradient Descent"""

def prox_grad(f,lmbda,D,alpha,niter):
    nu = np.zeros(D.shape[0])
    hist = np.zeros((niter,2))

    P = lambda nu : np.piecewise(nu, [np.abs(nu) <= lmbda, np.abs(nu) > lmbda], 
                                 [lambda x : x, lambda x : lmbda*np.sign(x)])

    for k in range(0,niter):
        nu = P(nu - alpha*D@(D.T@nu - f))
        u = f - D.T@nu
        # primal = 0.5*np.linalg.norm(u - f)**2 + lmbda*np.linalg.norm(D@u,ord=1)
        # dual = -0.5*np.linalg.norm(D.T@nu) + f.dot(D.T@nu)
        # hist[k] = primal, dual

    return u

# grid \Omega = [0,1]
n = 28
h = 1/(n-1)
x = np.linspace(0,1,n)

# parameters
niter = 10000
lmbda = 1


#Compute the grad
D_xy = np.gradient(x_train_blurry[id].reshape(28,28))
D_x  = D_xy[0]
D_y  = D_xy[1]
D = np.sqrt(np.abs(D_x)**2+ np.abs(D_y)**2)

# proximal gradient on dual problem
alpha = 1/np.linalg.norm(D)**2
u_prox = prox_grad(f_delta,lmbda,D,alpha,niter)

# plot
fig,ax = plt.subplots(1,2)

plt.gray()

ax[0].imshow(f_delta)
ax[0].set_title('Blurred image')
ax[0].set_xticks([])
ax[0].set_yticks([])

ax[1].imshow(u_prox)
ax[1].set_title('Result Prox')
ax[1].set_xticks([])
ax[1].set_yticks([])

"""### Admm"""

def admm(f,lmbda,D,rho,niter):
    m,n = D.shape
    nu = np.zeros(m)
    v = np.zeros(m)
    u = np.zeros(n)
    hist = np.zeros((niter,2))

    T = lambda v : np.piecewise(v, [v < -lmbda/rho, np.abs(v) <= lmbda/rho, v > lmbda/rho], 
                                [lambda x : x + lmbda/rho, lambda x : 0, lambda x : x - lmbda/rho])

    for k in range(0,niter):
        u = np.linalg.solve(np.eye(n) + rho*D.T@D, f + D.T@(rho*v - nu))
        v = T(D@u + nu/rho)
        nu = nu + rho*(D@u - v)

        # primal = 0.5*np.linalg.norm(u - f)**2 + lmbda*np.linalg.norm(D@u,ord=1)
        # dual = -0.5*np.linalg.norm(D.T@nu) + f.dot(D.T@nu)
        # hist[k] = primal, dual

    return u

# ADMM
rho = 1
u_admm = admm(f_delta,lmbda,D,rho,niter)

# plot
fig,ax = plt.subplots(1,2)

plt.gray()

ax[0].imshow(f_delta.reshape(28,28))
ax[0].set_title('Blurred image')
ax[0].set_xticks([])
ax[0].set_yticks([])

ax[1].imshow(u_admm.reshape(28,28))
ax[1].set_title('Result ADMM')
ax[1].set_xticks([])
ax[1].set_yticks([])